# Project Journal: Campus GPS Regression
**Date:** Dec 26, 2025

## What We Built Today
We built a Deep Learning pipeline that looks at a photo of campus and predicts its exact real-world location (Latitude/Longitude).
*   **Input:** ~350 photos of Building 35.
*   **Output:** GPS coordinates (meters from a center point).
*   **Model:** ResNet-18 (Transfer Learning).

---

## The Pipeline

### 1. Data Processing
*   **Image Standardization:** We convert all raw HEIC photos to standard JPGs and resize them to 256x256 pixels for efficiency.
*   **Coordinate Normalization:** Since neural networks struggle with large numbers like `31.26201`, we calculate a "Center of the World" (Average Lat/Lon) and convert all GPS tags into relative **(X, Y) meters** from that center.

### 2. Dataset Strategy (70/15/15 Split)
We strictly split our 350 images into three groups to ensure valid testing:
*   **Training (70%):** The data the model learns from.
*   **Validation (15%):** Used to tune the model and save the best version.
*   **Test (15%):** Locked away in a "vault" and used *only* for the final performance check.

### 3. The Model & Training
*   **Architecture:** We use a pre-trained **ResNet-18** backbone (which already knows how to "see" features) and replaced the final layer to output 2 coordinate numbers instead of classification classes.
*   **Handling Noisy Data:** We experimented with different ways to handle GPS inaccuracy.
    *   *Manual Filtering:* Initially, we considered manually removing or adjusting "bad" data points.
    *   *Weighted Loss:* We moved to a more robust **Inverse Variance Weighting** strategy. We use the phone's **GPS Accuracy** metadata to dynamically weight the loss. We punish the model heavily for missing precise photos, but are lenient if the original GPS data was fuzzy (e.g., 20m accuracy).
*   **Epoch Tuning:** We experimented with training duration.
    *   **20 Epochs:** Good baseline.
    *   **30 Epochs:** Optimal performance (best validation loss).
    *   **40 Epochs:** Performance degraded (overfitting started), returning to or worsening beyond the 20-epoch baseline.

### 4. Evolution of Validation Strategy
*   **Initial Approach:** We initially used the Weighted Loss for validation as well. While this showed us how well the model fit the "trusted" data, it was not a realistic test of real-world performance.
*   **Refinement:** We switched the Validation phase to use **Standard (Unweighted) MSE**. In the real world (inference time), the model won't have GPS metadata to rely onâ€”it must perform well on any image, regardless of how accurate the ground truth was during collection. This gives us a much more honest estimate of the error in meters.

---

## Future Improvements & Ideas
*   **Data Augmentation:** Introduce random rotations, brightness changes, and crops to make the model robust to different lighting conditions and angles.
*   **Hyperparameter Tuning:** Experiment with different learning rates, batch sizes, and number of epochs to find the sweet spot for convergence.
*   **Spatial Transformer Networks (STN):** Add a learnable STN layer that can automatically rotate or warp the input image to a canonical "front-facing" view before it hits the ResNet.
*   **Deeper Backbones:** Try ResNet-50 or EfficientNet to see if a larger model can capture more subtle visual details.

---

## Final Results
On completely unseen photos (the Test Set), the model predicts the location with an average error of **~7.5 meters**. Not bad for a first try on a small dataset!

---

**Date:** Dec 27, 2025

## Expanded Dataset & Blind Test
We significantly expanded our dataset by adding photos from multiple new areas of campus (`LibraryArea`, `UnderBuilding26`, `Building28Area`, `Bulding32Area`, `nightWithout26AndLibrary`, etc.). The dataset grew from ~350 images to **1,872 images**.

### Updates
1.  **Data Pipeline:** Updated `convert_images.py` and `normalize_coords.py` to handle nested folder structures and potential filename collisions by namespacing files with their parent folder names.
2.  **Quality Control:** Added automatic filtering to drop samples with missing GPS coordinates (1 sample dropped).
3.  **Blind Testing:** We maintained the **70/15/15 split** strategy. The split was performed randomly across the entire pooled dataset, ensuring the model was tested on images from various sections without any explicit knowledge of the section labels.

### Results
*   **Run 1 (25 Epochs):** Test Mean Error: **29.25 meters** | Test Median Error: **23.05 meters**
*   **Run 2 (50 Epochs):** Test Mean Error: **26.39 meters** | Test Median Error: **22.39 meters**
*   **Run 3 (Day Only, 25 Epochs):** Test Mean Error: **27.34 meters** | Test Median Error: **22.58 meters**
*   **Run 4 (Area-Specific Z-Score Weighting, 25 Epochs):** Test Mean Error: **32.23 meters** | Test Median Error: **28.09 meters**

### Experiment: Area-Specific Z-Score Weighting
We hypothesized that since different areas of campus have different baseline GPS variances, we should penalize outliers *relative to their specific area* rather than using a global standard.
*   **Implementation:** We calculated the Mean and Standard Deviation of the GPS Accuracy for *each* folder (area). We then calculated a Z-Score for every image: $Z = (\text{Accuracy} - \mu_{area}) / \sigma_{area}$.
*   **Weighting:** We introduced a penalty factor $R = 1 / (1 + \max(0, Z))$, reducing the influence of local outliers.
*   **Outcome:** The performance **decreased** (Mean Error rose from ~29m to ~32m). This suggests that the standard weighting (trusting good absolute GPS values regardless of area) is more effective. Penalizing "relative" outliers might have suppressed useful training signals in difficult areas where high-variance GPS is the best we have.

### Visualization Tool
We developed a visualization script (`src/utils/visualize_results.py`) to qualitatively analyze model performance.
*   **Features:** It generates a grid of test samples showing:
    *   The test image.
    *   The GPS Error in meters (color-coded).
    *   A mini-map (OpenStreetMap) showing the True (Green) and Predicted (Red) locations.
*   **Sampling Strategy:** To get a balanced view, the tool selects:
    *   3 Best Predictions (Lowest Error).
    *   3 Random Predictions.
    *   6 Worst Predictions (Highest Error).
*   **Purpose:** This allows us to visually inspect if specific conditions (e.g., night time, specific buildings) correlate with high errors.

## Model Refinement & Breakthrough
We analyzed the failure modes and made two major changes to the training strategy:

### 1. Stratified Data Split (85/10/5)
We realized that a random 70/15/15 split might exclude entire small locations from the training set or test set. We switched to **Stratified Sampling** based on location (folder name) and increased the training data:
*   **Train (85%):** Maximize data exposure.
*   **Val (10%):** Sufficient for model selection.
*   **Test (5%):** ~94 images, ensuring every building/area is represented proportionally.

### 2. Dropping Weighted Loss
We analyzed the `gps_accuracy_m` metadata and found a huge disparity (2m vs 15m). The Inverse Variance Weighting ($1/\sigma^2$) was scaling weights by a factor of **~56x**. This meant "low accuracy" images (which were often still visually valid) were effectively ignored.
*   **Action:** We switched to standard **MSE Loss** for training, treating all images as equally ground truth.
*   **Result:** Immediate and significant improvement. The model stopped ignoring "noisy" data and learned robust features from the entire dataset.

### Latest Results (25 Epochs)
*   **Mean Error:** **19.75 meters** (Previous Best: 26.39m)
*   **Median Error:** **15.95 meters** (Previous Best: 22.39m)
*   **Improvement:** **~25% reduction in error.**

### 3. Adding Learning Rate Scheduler
We noticed the loss spiking in later epochs, suggesting the learning rate was too high for fine-tuning. We implemented `ReduceLROnPlateau` (factor=0.1, patience=3).
*   **Outcome (50 Epochs):**
    *   **Mean Error:** **18.01 meters**
    *   **Median Error:** **14.53 meters**
    *   **Validation Loss:** Stabilized around ~473, indicating ResNet-18 had reached its capacity.

### 4. Upgrade to EfficientNet-B0
Given the limited dataset (~1,600 images), we switched from ResNet-18 (25M params) to **EfficientNet-B0** (5.3M params). EfficientNet is less prone to overfitting and learns more robust features.
*   **Outcome (50 Epochs + Scheduler):**
    *   **Mean Error:** **13.45 meters** (Previous Best: 18.01m)
    *   **Median Error:** **10.95 meters** (Previous Best: 14.53m)
    *   **Validation Loss:** Dropped massively to **~166** (was ~473).
*   **Status:** The model was still improving at Epoch 50, suggesting further training could break the 10-meter barrier.

### 5. Overfitting at 80 Epochs
We attempted to push the EfficientNet-B0 model further by training for 80 epochs.
*   **Outcome:** Performance regressed.
    *   **Mean Error:** **17.28 meters** (Worse than 13.45m)
    *   **Validation Loss:** Spiked to **328** (Doubled from 166).
*   **Analysis:** The model began to overfit the small training set after ~50 epochs. It memorized the training data instead of generalizing.
*   **Conclusion:** Simply training longer is not the solution. We need to make the model smarter (better architecture) or give it better data (higher resolution).

### 6. Failure of ConvNeXt-Tiny (On Mac)
We attempted to upgrade to **ConvNeXt-Tiny** (28M params) to capture better scene features.
*   **Outcome:** Training failed to converge. Loss exploded to ~4000+.
*   **Reason:** The model is too complex for the small batch size and limited compute available on the Mac (MPS). It likely needs hyperparameter tuning (warmup, weight decay) and larger batches.

### 7. Data Augmentation Experiment
We reverted to **EfficientNet-B0** but added robust data augmentation to prevent the overfitting we saw at epoch 80.
*   **Augmentations:** `RandomResizedCrop` (scale 0.8-1.0) and `ColorJitter`.
*   **Results (25 Epochs on Mac):**
    *   **Mean Error:** **18.48 meters**
    *   **Median Error:** **17.56 meters**
    *   **Analysis:** The results are worse than the non-augmented run (~13m), likely because the model needs more epochs to learn from harder data.

---

**Date:** Dec 30, 2025

## GPU Cluster Training & New Best Model

### Infrastructure Upgrade
We gained access to BGU's GPU cluster with a **NVIDIA GTX 1080 Ti** (CUDA). This allowed faster experimentation compared to local Mac training on MPS.

#### Cluster Setup Challenges
1.  **PyTorch CUDA Compatibility:** The latest PyTorch versions drop support for older GPU architectures. We installed `torch==1.12.1+cu116` for GTX 1080 Ti (CUDA 6.1 compute capability).
2.  **NumPy Version Conflict:** PyTorch 1.12 requires NumPy <2.0. Resolved by installing `numpy<2`.
3.  **Data Loading Bottleneck:** Initial training was slow (~5s/epoch) because the GPU was waiting for CPU data loading. Fixed with:
    *   `num_workers=4` for parallel data loading.
    *   `pin_memory=True` for faster GPU memory transfer.
    *   Increased batch size from 16 â†’ 32.

### Smart Checkpoint Management
We implemented a robust checkpointing system that stores `best_loss` alongside the model weights. The training script now:
1.  Loads the previous `best_loss` at startup.
2.  Only saves a new checkpoint if the current validation loss **beats** the previous best.
3.  Prevents accidentally overwriting a good model with a worse one from a subsequent run.

### Baseline Run on GPU (No Augmentation)
We ran a clean 50-epoch baseline with **EfficientNet-B0** and **no augmentation** to establish the true performance on GPU.

*   **Training:** 1590 samples, 50 epochs, batch size 32, LR scheduler (ReduceLROnPlateau).
*   **Validation Loss:** Converged to **170.9** at Epoch 25.
*   **Final Test Results:**
    *   **Mean Error:** **15.27 meters** â­ **(Current Best Saved Model)**
    *   **Median Error:** **15.10 meters**
*   **Observations:**
    *   The model improved rapidly in the first 25 epochs (loss: 3081 â†’ 170).
    *   After Epoch 25, the learning rate scheduler kicked in, and validation loss plateaued (~170-200), indicating the model reached its capacity without augmentation.
    *   The gap between the Mac run (13.45m) and GPU run (15.27m) is likely due to different random seeds and data loader shuffling.

---

## Higher Resolution & Model Experiments

### Image Reprocessing to 384Ã—384
We reprocessed all 1,873 HEIC images from raw to **384Ã—384** JPGs to support higher-resolution models. This took ~20 minutes on Mac.

### Experiment: EfficientNet-V2-S at 384Ã—384
We tried the modern **EfficientNet-V2-S** (21M params) with native 384Ã—384 input.

*   **Mac Training (25 Epochs):**
    *   Mean Error: **23.27m**, Median: **21.80m**
    *   Val Loss: 523.98
    *   Model was still learning, needed more epochs.

*   **GPU Training (Resumed, +50 Epochs):**
    *   Train Loss: ~108 (~10m error)
    *   Val Loss: ~452-490 (~21m error)
    *   Mean Error: **20.26m**, Median: **16.12m**

*   **Analysis:** Severe overfitting! Train/val gap of 2x indicates the model is memorizing, not generalizing.

### Diagnosing Overfitting
We identified multiple issues:
1.  **Too many parameters:** 21M params for 1,590 samples (rule of thumb: need 10-100 samples per param).
2.  **No augmentation:** Model saw identical images every epoch.
3.  **Wrong augmentation:** `RandomHorizontalFlip` was enabled but is invalid for GPS regression (would need to flip X coordinate too).

### Fixes Implemented
1.  **Enabled data augmentation:** `ColorJitter`, `RandomGrayscale`, `RandomRotation(Â±5Â°)`.
2.  **Removed `RandomHorizontalFlip`:** Invalid for GPS regression.
3.  **Added weight decay:** `weight_decay=1e-4` for L2 regularization.
4.  **Added resume flag:** Training can now continue from checkpoints with `resume=True`.

### Experiment: EfficientNet-B3 at 300Ã—300 with Augmentation
We switched to **EfficientNet-B3** (12M params, native 300Ã—300) - a balance between capacity and overfitting risk.

*   **GPU Training (50 Epochs):**
    *   Best Val Loss: **202.96** (at Epoch 14)
    *   Train Loss: ~140-150 (plateaued after epoch 20)
    *   Val Loss: ~211-215 (plateaued after LR reduction)

*   **Test Results:**
    *   Mean Error: **14.33 meters** â­ **(New Best!)**
    *   Median Error: **12.75 meters**

*   **Observations:**
    *   The train/val gap is much smaller (~140 vs ~200) - less overfitting!
    *   LR scheduler kicked in at epoch 14 (patience=3 was too aggressive).
    *   Model plateaued for 30+ epochs after LR reduction.

### Why Training Plateaued
| Epoch | Event |
|-------|-------|
| 1-14 | Rapid learning, val loss: 1819 â†’ 202 |
| 15-18 | Val loss spiked (476, 610), LR scheduler triggered |
| 19-45 | LR reduced 10x (0.001 â†’ 0.0001), stuck at ~211-215 |

The `ReduceLROnPlateau` with `patience=3` reduced learning rate too aggressively. The model couldn't escape its local minimum.

---

## Expert Friend's Suggestions & Best Model Yet

### The Challenge
After extensive experimentation, we were stuck at ~14m mean error. The key issues:
1.  **Model size vs. data:** Larger models (V2-S, B3) overfit despite augmentation.
2.  **Scheduler too aggressive:** `patience=3` killed learning prematurely.
3.  **Suboptimal loss function:** MSE squares errors, making outliers dominate.

### Expert Recommendations Implemented
We received detailed suggestions from an ML expert and implemented all of them:

#### 1. Architecture: EfficientNet-B0 + MLP Head
**Why:** B0 has only ~4M parameters, well-matched to our 1,500 training samples (rule of thumb: ~100-1000 samples per million params). The custom MLP head handles the non-linearity of GPS regression better than a single linear layer.

```python
self.backbone.classifier = nn.Sequential(
    nn.Linear(in_features, 512),
    nn.ReLU(),
    nn.Dropout(0.3),
    nn.Linear(512, 128),
    nn.ReLU(),
    nn.Linear(128, 2)
)
```

#### 2. Image Resolution: 256Ã—256
**Why:** Higher resolution (384Ã—384) didn't improve results for B0 and just slowed training. 256Ã—256 is EfficientNet-B0's native resolution and captures sufficient detail for campus landmarks.

*Action:* Reprocessed all images to `processed_images_256/`.

#### 3. Loss Function: HuberLoss (Î´=1.0)
**Why:** MSE squares all errors, so a single 50m outlier dominates the loss. HuberLoss is quadratic for small errors but **linear** for large ones, making it robust to noisy GPS ground truth.

```python
criterion = nn.HuberLoss(delta=1.0)
```

*Effect:* Loss values are much smaller (7.5 vs 200+) because we're not squaring large errors.

#### 4. Data Augmentation (Revised)
**Why:** Previous augmentation included `RandomHorizontalFlip` which is **invalid for GPS regression** (flipping an image doesn't flip the X coordinate in our labels).

**New Pipeline:**
| Transform | Reason |
|-----------|--------|
| `RandomPerspective(0.3, p=0.5)` | Simulates different camera angles |
| `ColorJitter(0.2, 0.2, 0.2, 0.05)` | Handles lighting variation |
| `RandomGrayscale(p=0.1)` | Color-invariant features |
| `RandomRotation(Â±5Â°)` | Slight orientation changes |
| `RandomErasing(p=0.2)` | Simulates occlusions |

**Removed:** `RandomHorizontalFlip`, `RandomResizedCrop` (too aggressive).

#### 5. Training Hyperparameters
| Parameter | Old | New | Why |
|-----------|-----|-----|-----|
| Learning Rate | 1e-3 | 1e-3 | Keep aggressive start |
| Weight Decay | 1e-4 | 1e-4 | L2 regularization |
| Scheduler Patience | 3 | **7** | Let model explore longer before reducing LR |
| Scheduler Factor | 0.1 | **0.5** | Less aggressive reduction |
| Epochs | 50 | **75** | More time to converge |

#### 6. Data Split: 80/10/10
**Why:** The previous 5% test set (~94 samples) was too small for reliable metrics. Increased to 10% (188 samples) while maintaining stratified sampling across all campus locations.

---

### Results: Best Model! ðŸ†

**Experiment:** `b0_256_mlp` (75 epochs on GTX 1080 Ti)

| Metric | Value | Previous Best |
|--------|-------|---------------|
| **Mean Error** | **11.94 meters** | 14.33m (B3) |
| **Median Error** | **9.05 meters** â¬‡ï¸ | 12.75m (B3) |
| Best Val Loss | 7.50 (HuberLoss) | 202.96 (MSE) |

*Note: Loss values aren't directly comparable due to different loss functions.*

### Key Observations

1.  **Continuous Improvement:** The model saved its best checkpoint on the **final epoch (75)**, meaning it was still learning! No premature plateau.

2.  **Healthy Train/Val Gap:** Train loss (4.3) vs Val loss (7.5) shows ~2x gap. Some overfitting, but much healthier than V2-S (10x gap).

3.  **Median Under 10m!** The median error of **9.05m** breaks the 10-meter barrier for the first time. This means over half of predictions are within 10 meters.

### Visualization Analysis
Looking at the 50-sample visualization:
*   **Best cases (0.5-3m):** The model correctly identifies distinctive buildings with unique architecture.
*   **Worst cases (30-50m+):** Mostly night images and generic hallways/walkways that lack distinctive features.
*   **Night images remain challenging:** Many of the worst predictions are dark/low-light scenes.

---

### Resume Training (Epochs 76-94)

We resumed training from the epoch 75 checkpoint to see if further improvement was possible.

**Results:**
| Epoch | Train Loss | Val Loss | Notes |
|-------|------------|----------|-------|
| 75 | 4.29 | 7.50 | Starting point |
| 77 | 4.32 | **7.10** | New best! âœ“ |
| 88 | 3.88 | 7.26 | Train improving, val stuck |
| 94 | 3.89 | 7.18 | Stopped (no improvement) |

**Best Val Loss:** 7.0977 (at epoch 77)

### Conclusion: Resuming Training Provides Diminishing Returns

The model has reached its capacity with the current architecture and data:

1.  **Widening Train/Val Gap:** Train loss continued dropping (4.3 â†’ 3.8) while val loss stayed flat (~7.1-7.3). This indicates the model is memorizing training data rather than learning generalizable features.

2.  **Marginal Improvement:** Only 0.4 loss improvement (7.50 â†’ 7.10) over 19 epochs. The model saved once at epoch 77 and never improved again.

3.  **Diminishing Returns:** Further training would only increase overfitting without improving test accuracy.

**Final Test Results (Epoch 77 Checkpoint):**
| Metric | Value |
|--------|-------|
| Mean Error | **11.63 meters** |
| Median Error | **8.85 meters** |
| Val Loss | 7.10 (HuberLoss) |

**Key Finding:** Resuming training from a converged model yields minimal gains. The EfficientNet-B0 + MLP architecture with 256Ã—256 images has extracted all the learnable signal from our ~1,500 training images. To improve further, we need either **more data** or **better data quality**.

---

### Worst-Case Analysis

We analyzed the 25 worst predictions to identify failure patterns:

| Pattern | Count | Percentage |
|---------|-------|------------|
| Night/low-light images | 12/25 | 48% |
| Generic walkways/corridors | 8/25 | 32% |
| Building 32 Area | 6/25 | 24% |
| Library Area (open spaces) | 5/25 | 20% |

**Key Insights:**
1.  **Night images are the biggest problem** - Nearly half the worst predictions are dark scenes lacking distinctive features.
2.  **Generic architecture confuses the model** - Covered walkways, concrete pillars, and open plazas look identical across campus.
3.  **Some might be GPS label errors** - A few images show distinctive buildings but have huge errors, suggesting incorrect ground truth.

---

## Paths Forward for Sub-10m Mean Error

### Option 1: Light GPS Accuracy Weighting (New Idea)
We have a `gps_accuracy_m` column in the dataset (manually corrected: all 15m+ values set to 7m). We could apply **very light weighting** based on this:

```python
# Example: weight = 1 / (1 + 0.02 * accuracy)
# accuracy = 2m  â†’ weight = 0.96
# accuracy = 7m  â†’ weight = 0.88
# accuracy = 15m â†’ weight = 0.77
# Ratio of best to worst: only ~1.25x (not 56x like before!)
```

**Why this might help:**
*   Gives slight preference to high-confidence GPS labels during training.
*   Doesn't ignore manually-corrected samples (unlike our original 1/ÏƒÂ² weighting).
*   The model can still learn from all data, just with subtle guidance.

### Option 2: Data Quality
The GPS ground truth has inherent noise. Some images may have incorrect labels.

*   **Manual Review:** Check the worst predictions - are they model errors or label errors?
*   **Use Model to Find Bad Labels:** High training loss on a sample = potential mislabel.

### Option 3: More Training Data
With ~1,500 samples and 4M parameters, we're near the data limit.

*   **Collect More Photos:** Especially night scenes and underrepresented areas.
*   **Stronger Night Augmentation:** More aggressive brightness/contrast variation.

### Option 4: Architectural Improvements
*   **Two-Stage Approach:** First classify the region, then regress within it.
*   **Attention Mechanisms:** Help model focus on distinctive landmarks.

### Option 5: Ensemble
*   Train 3-5 models with different random seeds.
*   Average predictions to reduce variance (typically 5-10% improvement).

---

### Current Best Model Summary (Pre-Ensemble)
| Experiment | Mean Error | Median Error | Val Loss |
|------------|------------|--------------|----------|
| `b0_256_mlp` (epoch 77) | **11.63m** | **8.85m** | 7.10 |

---

## Ensemble Approach: Sub-10m Mean Error Achieved! ðŸ†

**Date:** Dec 30, 2025 (continued)

After hitting the limit with a single model (~11.6m mean error), we implemented an **ensemble of 3 models** trained with different random seeds. This is a classic variance-reduction technique that often provides 5-15% improvement.

### Changes Implemented

#### 1. Night Simulation Augmentation
We identified that **48% of worst predictions were night/low-light images**. To address this, we added aggressive "night mode" simulation:

```python
transforms.RandomApply([
    transforms.ColorJitter(brightness=(0.1, 0.4), contrast=(0.1, 0.4), saturation=0.1, hue=0.01),
], p=0.25)  # 25% of training images brutally darkened
```

This forces the model to learn features that work in low-light conditions.

#### 2. Label Audit & GPS Corrections
We created a tool (`export_worst_for_audit.py`) to export the worst 25 predictions to a CSV with Google Maps links. Manual inspection revealed **3 incorrect GPS labels** in the dataset:

| Image | Original Error | Issue | Correction |
|-------|---------------|-------|------------|
| `Bulding32Area_IMG_7899.jpg` | 50.9m | Wrong location | Fixed GPS |
| `Building35Lower_IMG_7631.jpg` | 36.7m | Wrong location | Fixed GPS |
| `LibraryArea_IMG_3014.jpg` | 45.6m | Retained (edge case) | - |

Corrections were applied directly to `dataset.csv`.

#### 3. Seed Parameter for Reproducibility
Modified `train.py` to accept a `seed` parameter that sets:
- `torch.manual_seed(seed)`
- `np.random.seed(seed)`
- `random.seed(seed)`
- `torch.cuda.manual_seed(seed)` and `torch.cuda.manual_seed_all(seed)`

#### 4. Ensemble Evaluation Tools
Created multiple scripts:
- `ensemble_evaluate.py`: Loads N models, averages predictions, reports metrics
- `visualize_ensemble.py`: Visualizes ensemble predictions on test set
- `visualize_worst_ensemble.py`: Shows only worst 25 ensemble predictions
- `predict.py`: Standalone TA submission script for single-image prediction

### Training: 3 Models with Different Seeds

| Model | Seed | Best Val Loss | Training Epochs |
|-------|------|---------------|-----------------|
| `b0_ensemble_s42` | 42 | 6.97 | 88 + 15 = 103 |
| `b0_ensemble_s100` | 100 | 7.52 | 87 + 11 = 98 |
| `b0_ensemble_s123` | 123 | 7.28 | 85 + 15 = 100 |

All three models used identical architecture (EfficientNet-B0 + MLP head), same hyperparameters, and same augmentation. Only the random seed differed.

### Results: Sub-10m Mean Error! â­

| Metric | Single Best Model | **Ensemble (3 models)** | Improvement |
|--------|-------------------|------------------------|-------------|
| **Mean Error** | 10.53m | **9.83m** | -6.6% |
| **Median Error** | 8.74m | **7.18m** | -17.8% |
| **Worst Error** | 45.6m | **40.7m** | -10.7% |

### Why Ensembles Work

1. **Variance Reduction:** Each model learns slightly different features due to random initialization. Averaging smooths out individual errors.

2. **Robust to Outliers:** If one model makes a bad prediction, the other two can "vote it down."

3. **No Additional Data Needed:** We extracted more signal from the same 1,500 training images.

### Worst Predictions Analysis (Ensemble)

| Pattern | Count | Percentage |
|---------|-------|------------|
| Night/low-light images | 10/25 | **40%** |
| Building 32 Area | 5/25 | 20% |
| Generic corridors/walkways | 5/25 | 20% |
| Library Area | 4/25 | 16% |

**Key Finding:** Night images remain the biggest challenge, but the ensemble reduced their impact. The worst error dropped from 45.6m â†’ 40.7m.

### Observations from Extended Training

After initial 73-75 epochs, we resumed each model for 15 more epochs:

| Model | Epochs | Improvement |
|-------|--------|-------------|
| s42 | 88â†’103 | No new best (stayed at 6.97) |
| s100 | 83â†’98 | Small improvement (7.55â†’7.52) |
| s123 | 85â†’100 | Small improvement (7.36â†’7.28) |

**Conclusion:** The models have converged. Further training yields diminishing returns. The train/val gap (~3.5 vs ~7.2) indicates mild overfitting.

---

## Final Results Summary

| Approach | Mean Error | Median Error | Notes |
|----------|------------|--------------|-------|
| ResNet-18 (initial) | 29.25m | 23.05m | First attempt |
| EfficientNet-B0 (50 epochs) | 13.45m | 10.95m | Better backbone |
| EfficientNet-B0 + MLP + HuberLoss | 11.63m | 8.85m | Expert recommendations |
| **Ensemble (3 seeds)** | **9.83m** | **7.18m** | ðŸ† **Best Result** |

### Achievement: Sub-10m Mean Error âœ…
The goal of achieving **sub-10 meter mean error** has been accomplished through:
1. Right-sized model (EfficientNet-B0, 4M params)
2. Custom MLP regression head
3. HuberLoss for outlier robustness
4. Night simulation augmentation
5. Label audit and GPS corrections
6. Ensemble of 3 models with different seeds

---

## Paths Forward (If Continued)

### 1. More Night Training Data
Night images account for 40% of worst predictions. Collecting more night photos would directly address this.

### 2. Test-Time Augmentation (TTA)
For each test image, run inference on multiple augmented versions (rotations, brightness variations) and average. This is like an ensemble at inference time.

### 3. Region Classification â†’ Fine Regression
Two-stage approach: First classify the broad region (Library, Building 32, etc.), then use a region-specific model for fine-grained localization.

### 4. Uncertainty Estimation
Train models to output both prediction AND uncertainty. High-uncertainty predictions could trigger fallback behavior.

### 5. Light GPS Accuracy Weighting
We have `gps_accuracy_m` in the dataset but currently ignore it. A **light weighting** could help:

```python
# weight = 1 / (1 + 0.02 * accuracy)
# accuracy = 2m  â†’ weight = 0.96
# accuracy = 7m  â†’ weight = 0.88
# accuracy = 15m â†’ weight = 0.77
# Ratio of best to worst: ~1.25x (not 56x like original!)
```

This gives slight preference to high-confidence GPS labels without ignoring low-confidence ones.

---

### Final Model Summary
| Metric | Value |
|--------|-------|
| Architecture | EfficientNet-B0 + MLP Head |
| Input Resolution | 256Ã—256 |
| Loss Function | HuberLoss (Î´=1.0) |
| Ensemble Size | 3 models |
| **Mean Error** | **9.83 meters** |
| **Median Error** | **7.18 meters** |
| Models Saved | `best_b0_ensemble_s42.pth`, `best_b0_ensemble_s100.pth`, `best_b0_ensemble_s123.pth` |

---

**Date:** Dec 31, 2025

## Data Pipeline Cleanup & Documentation

### Streamlined Data Flow

We reorganized the data preparation pipeline to support incremental photo additions and automatic GPS correction application:

```
raw_photos/
    â””â”€â”€ Building26Night/
    â””â”€â”€ LibraryNight/
    â””â”€â”€ ... (new folders)
            â”‚
            â–¼
    extract_metadata.py --folders Building26Night LibraryNight
            â”‚
            â–¼
    metadata_raw/*.csv  (raw GPS from EXIF)
            â”‚
            â–¼
    normalize_coords.py  (auto-applies corrections_batch*.csv)
            â”‚
            â–¼
    dataset.csv  (final training data with all corrections)
```

### Key Scripts

#### `extract_metadata.py`
Extracts GPS coordinates from raw HEIC photos using EXIF data.

```bash
# Extract all folders (default)
python extract_metadata.py

# Extract only specific new folders (incremental)
python extract_metadata.py --folders Building26Night LibraryNight
```

**Output:** Creates one CSV per folder in `data/metadata_raw/`

#### `normalize_coords.py`
Merges all metadata, converts lat/lon to local X/Y meters, and **automatically applies all corrections**.

```bash
python normalize_coords.py
```

**Output:**
```
Found 9 CSV files: ['Building28Area.csv', 'LibraryArea.csv', ...]
Total samples loaded: 1873

Applying corrections from 4 file(s)...
  corrections_batch1.csv: 62 corrections
  corrections_batch2.csv: 62 corrections
  corrections_batch3.csv: 61 corrections
  corrections_batch4.csv: 3 corrections
  Total updated: 188 samples

Saved to dataset.csv
```

### GPS Correction Workflow

When model predictions reveal potential GPS label errors, use this workflow to fix them:

#### Step 1: Export Worst Predictions
```bash
python src/utils/export_worst_for_audit.py --num 25
```
Creates `outputs/worst_for_maps.csv` with simple format: `name, latitude, longitude`

#### Step 2: Review in Google Maps
1. Go to [mymaps.google.com](https://mymaps.google.com)
2. Create new map â†’ Import â†’ Select `worst_for_maps.csv`
3. Drag points to correct locations
4. Export or note the corrected coordinates

#### Step 3: Import Corrections
```bash
python src/utils/import_corrections.py corrected_coords.csv --output corrections_batch5.csv
```
Converts simple coordinate CSV to corrections format.

#### Step 4: Regenerate Dataset
```bash
python src/data_prep/normalize_coords.py
```
Applies all corrections (batch1-5) and regenerates `dataset.csv`.

### Correction File Format (Simplified)

Corrections now use a minimal 5-column format:

```csv
filename,path,lat,lon,gps_accuracy_m
IMG_7146.HEIC,data/raw_photos/UnderBuilding26/IMG_7146.HEIC,31.2620788,34.8023207,7.0
```

The `normalize_coords.py` script only reads `path`, `lat`, `lon` columns - other fields are optional metadata.

### Why This Matters

1. **Incremental Updates:** When adding new photos, only extract metadata from new folders - existing corrections are preserved.

2. **Automatic Corrections:** Running `normalize_coords.py` always applies all `corrections_batch*.csv` files, so regenerating `dataset.csv` never loses fixes.

3. **Audit Trail:** Each correction batch is a separate file (`corrections_batch1.csv`, `corrections_batch2.csv`, etc.) documenting when and what was fixed.

---

## Augmentation Tuning

### Visualization Tool

Created `src/utils/visualize_augmentations.py` to preview augmentation effects before training:

```bash
# Show 10 images with 10 augmentations each
python src/utils/visualize_augmentations.py --images 10 --augs 10

# Test experimental settings
python src/utils/visualize_augmentations.py --images 10 --augs 10 --experimental
```

Generates a grid comparing original images against augmented versions, saved to `outputs/augmentation_examples.png`.

### Revised Augmentation Settings

After visual inspection of 100+ augmented samples, we tuned the augmentation pipeline:

| Parameter | Old Value | New Value | Reason |
|-----------|-----------|-----------|--------|
| ColorJitter brightness | 0.2 | **0.3** | More lighting variation |
| ColorJitter saturation | 0.2 | **0.3** | Slightly more color range |
| ColorJitter hue | 0.05 | **0.1** | Simulates different times of day (warm/cool) |
| Night brightness | (0.1, 0.4) | **(0.4, 0.7)** | Old was too dark (nearly black) |
| Night contrast | (0.1, 0.4) | **(0.6, 0.9)** | Preserve structure visibility |
| Night probability | 25% | **20%** | Slightly less frequent |

**Key insight:** The original night simulation (10-40% brightness) produced nearly black images where the model couldn't learn anything. The new settings (40-70% brightness) create challenging but learnable low-light conditions.

### Geometric Augmentations Refined

After careful consideration of what augmentations are valid for GPS regression:

| Augmentation | Status | Reasoning |
|--------------|--------|-----------|
| `RandomRotation(Â±5Â°)` | **Added** | TA mentions "some variation in angle may occur" in test images |
| `RandomPerspective(0.2)` | **Kept (reduced)** | Simulates phone tilt (same location, different angle). Reduced from 0.3 to 0.2 for gentler effect |
| `RandomHorizontalFlip` | **Removed** | Invalid - flipping image doesn't flip GPS X coordinate |
| `RandomResizedCrop` | **Not used** | Could crop out distinctive landmarks |

---

**Date:** Jan 1, 2026

## Test Set Restructure

### Problem with Previous Approach
The old 80/10/10 split meant the test set came from the same photo sessions as training data. This could lead to overly optimistic results if similar photos ended up in both sets.

### New Approach: External Test Set
We collected a dedicated **TestPhotos** folder (1,322 images) from a separate photo session covering the entire campus. This is a true held-out test set that the model has never seen during training.

### Data Split Summary

| Dataset | Count | Purpose |
|---------|-------|---------|
| **Training pool** | 1,950 | 85/15 train/val split |
| **Test set** | 1,321 | External evaluation (TestPhotos) |
| **Night holdout** | 54 | Night-specific evaluation (10% of night photos) |

### Files Created

| File | Description |
|------|-------------|
| `data/dataset.csv` | Training data only (excludes test and night holdout) |
| `data/test_dataset.csv` | External test set from TestPhotos |
| `data/night_holdout.csv` | 10% of night photos for separate evaluation |

### Code Changes

**`normalize_coords.py`**: Now creates three CSV files:
- Identifies TestPhotos by `source_file == 'TestPhotos.csv'`
- Randomly selects 10% of night photos for holdout (seed=42)
- Remaining data goes to training pool

**`train.py`**: Simplified to 85/15 train/val split:
- No more test split (external test set used instead)
- Stratified by location (source_file)
- Removed `test_indices.npy` saving

**`evaluate.py`**: Loads `test_dataset.csv` directly:
- No longer uses indices from training
- Evaluates on completely external data

**`evaluate_night.py`** (NEW): Separate night evaluation:
- Loads `night_holdout.csv`
- Reports night-specific metrics (mean, median, distribution)

### Usage

```bash
# Train with 85/15 split
python -c "from src.training.train import train_model; train_model(num_epochs=75, experiment_name='v2')"

# Evaluate on external test set
python src/training/evaluate.py v2

# Evaluate on night holdout
python src/utils/evaluate_night.py v2
```

---

## Jan 1, 2026 (Evening) - Training Results & Problematic Photos Extraction

### Training Run: b0_256_v2 (114 epochs total)

Trained EfficientNet-B0 with the new external test set structure:
- **85 epochs** initial run
- **+15 epochs** resume (to epoch 99)
- **+15 epochs** resume (to epoch 114)

**Validation Loss Progression:**
- Epoch 1: 46.17 â†’ Epoch 85: 6.50 â†’ Epoch 99: 5.94 â†’ Epoch 114: 5.82

Model kept improving throughout - no overfitting detected.

### Evaluation Results

**Test Set (1,321 samples from TestPhotos):**
| Metric | Value |
|--------|-------|
| Mean Error | 17.48 m |
| Median Error | 13.37 m |

**Night Holdout (54 samples):**
| Metric | Value |
|--------|-------|
| Mean Error | 10.58 m |
| Median Error | 8.07 m |
| Under 10m | 55.6% |
| Under 20m | 85.2% |

**Key Insight:** Night holdout has lower error, but this is likely due to sample size (54 vs 1,321) rather than better night performance. With fewer samples from a limited area, there are simply fewer opportunities to encounter blind spots. The test set's larger size and geographic diversity exposed more failure modes.

### Error Distribution Analysis

| Error Range | Count | % |
|-------------|-------|---|
| Under 10m | 460 | 34.8% |
| Under 20m | 940 | 71.2% |
| Under 30m | 1,154 | 87.4% |
| **Over 30m** | 167 | 12.6% |
| Over 50m | 40 | 3.0% |
| Over 100m | 9 | 0.7% |

### Problematic Photos Extraction

The 167 photos with >30m error were "blind spots" - areas/views not well-represented in training. Decision: **Move them to training data.**

**Created:** `src/data_prep/extract_problematic.py`
- Identifies test photos with error > threshold
- Renames files: `TestPhotos_*.jpg` â†’ `ProblematicPhotos_*.jpg`
- Moves raw photos to new folder
- Creates properly formatted metadata CSV
- Updates dataset.csv (training) and test_dataset.csv

**Results:**
| Dataset | Before | After |
|---------|--------|-------|
| Training Pool | 1,950 | 2,117 samples (+167) |
| Test Set | 1,321 | 1,154 samples (-167) |

### New Folder Structure

```
data/
â”œâ”€â”€ raw_photos/
â”‚   â”œâ”€â”€ ProblematicPhotos/     â† NEW (167 HEIC files)
â”‚   â”œâ”€â”€ TestPhotos/            â† (1,154 remaining)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ metadata_raw/
â”‚   â”œâ”€â”€ ProblematicPhotos.csv  â† NEW
â”‚   â””â”€â”€ ...
â””â”€â”€ processed_images_256/
    â”œâ”€â”€ ProblematicPhotos_*.jpg  â† (167 renamed)
    â””â”€â”€ TestPhotos_*.jpg         â† (1,154 remaining)
```

### Next Steps

1. **Retrain with expanded dataset** - The model should now learn the "blind spots"
2. **Evaluate on cleaner test set** - 1,154 samples without the extreme outliers
3. **Consider ensemble** - Train multiple models with different seeds

---

## Jan 1, 2026 (Night) - v3 Training Results: Major Improvement! ðŸŽ‰

### Training: b0_256_v3 (150 epochs)

Trained with the expanded dataset (2,117 samples including 167 ProblematicPhotos):

**Training Progression:**
- Epochs 1-100: Loss dropped from ~47 â†’ ~6.5
- Epochs 100-150: Fine-tuning, best val loss 6.20 at epoch 139
- Training loss reached ~3.0 while validation plateaued ~6.2-6.5

**Observation:** Validation loss slightly higher than v2 (6.20 vs 5.82), but this is just validation set variance - real test performance tells a different story!

### Evaluation Results: v3 vs v2 Comparison

| Metric | v2 (before) | v3 (after) | Change |
|--------|-------------|------------|--------|
| **Test Mean** | 17.48m | **11.54m** | **-34%** |
| **Test Median** | 13.37m | **9.90m** | **-26%** |
| Night Mean | 10.58m | 10.07m | -5% |
| Night Median | 8.07m | 7.51m | -7% |

### Error Distribution Improvement

| Range | v2 | v3 |
|-------|-----|-----|
| Under 10m | 34.8% | **50.8%** |
| Under 20m | 71.2% | **88.6%** |
| Under 30m | 87.4% | **97.7%** |
| Over 50m | 40 samples (3.0%) | **3 samples (0.3%)** |
| Over 100m | 9 samples | **0 samples** |

### Key Insights

1. **Problematic photos strategy worked!** Adding the 167 high-error photos to training allowed the model to learn those "blind spots"

2. **Validation loss â‰  test performance**: v3 had slightly higher validation loss than v2, but performed significantly better on the real test set. This confirms: small validation sets can be misleading.

3. **Catastrophic failures nearly eliminated**: 
   - Worst error: 171m â†’ 61m
   - Over-50m errors: 40 â†’ 3 (92% reduction!)

4. **Median under 10m achieved!** The median error of 9.90m means more than half of predictions are within 10 meters - a key milestone.

### Remaining Failure Patterns

Looking at the worst 25 test predictions (30-61m errors):
- Open plazas with generic architecture
- Long-distance building views
- Ambiguous corridors/pathways

These represent the inherent limits of the dataset/model rather than fixable blind spots.

### Final Model Summary

**Best Model: b0_256_v3**
- Architecture: EfficientNet-B0 + Custom MLP head
- Input: 256x256 images
- Training: 2,117 samples, 150 epochs
- Loss: HuberLoss (delta=1.0)
- Augmentation: ColorJitter, RandomPerspective, RandomRotation, Night simulation, RandomErasing

**Performance:**
- Test Mean: 11.54m
- Test Median: 9.90m
- 97.7% within 30m
- Only 0.3% catastrophic failures (>50m)

---

## Analysis: Error Patterns & Future Improvements

### Error Distribution Analysis

Analyzed 131 samples with >20m prediction error (11.4% of test set):

**Geographic Clustering:**
- High errors cluster along Yâ‰ˆ0 (campus center axis)
- Top error zones: (-20m, 0m), (0m, 0m), (-60m, 0m), (+40m, 0m)
- These areas likely have visually similar/ambiguous features

**Prediction Bias:**
- X bias: -2.9m, Y bias: +6.3m (minimal systematic bias)
- Model isn't consistently predicting wrong direction

### Ideas for Further Improvement

#### 1. Higher Resolution Input (Currently 256x256)
| Resolution | Potential Benefit | Risk |
|------------|------------------|------|
| **320x320** | More fine details visible | Slight training slowdown |
| 384x384 | Maximum detail | Overfitting risk with current data size |

Recommendation: Try 320x320 as a balanced improvement.

#### 2. Another Round of Problematic Photos Extraction
- Currently 11.4% of samples have >20m error
- Could extract these and add to training (same technique as before)
- Would require reprocessing images and retraining

#### 3. Label Smoothing / Uncertainty Training
Add noise during training proportional to expected uncertainty:
```python
noisy_target = target + noise_scale * torch.randn_like(target)
```
This teaches the model that exact coordinates have inherent uncertainty.

#### 4. Focal Loss / Hard Example Mining
Weight training more heavily on difficult samples:
```python
sample_weight = 1 + previous_error * scale_factor
```
Forces model to focus on hard cases rather than easy ones.

#### 5. Test-Time Augmentation (TTA)
During inference, apply multiple augmentations and average predictions:
- Flip horizontally
- Small rotations
- Slight crops
Could reduce variance without retraining.

#### 6. Attention Mechanisms
Add attention to the MLP head to help model focus on distinctive features:
- Self-attention on CNN features
- Could help distinguish similar-looking locations

### Priority Order
1. **Problematic photos extraction** (proven technique)
2. **Higher resolution (320x320)** (low risk, potential gain)
3. **TTA** (no retraining needed)
4. **Focal loss** (training tweak)
5. **Attention** (architecture change, higher risk)

---

## Jan 1, 2026 (Evening): Killer Combo Strategy Implementation

### The Strategy
Combined three proven techniques for final push to sub-10m error:
1. **Round 2 Problematic Photos** - Extract 131 photos with >20m error from test set
2. **Resolution Bump** - 256x256 â†’ 320x320 (+56% pixels)
3. **Ensemble** - Train 3 models with different seeds

### Changes Made
- Extracted 131 problematic photos (error >20m) â†’ `ProblematicPhotos2/`
- Training set: 2,117 â†’ 2,248 samples
- Test set: 1,154 â†’ 1,023 samples
- Reprocessed all images at 320x320 resolution
- Updated `INPUT_SIZE = 320` in dataset.py
- Training 3 models: `b0_320_seed42`, `b0_320_seed123`, `b0_320_seed456`

### Technical Note: Validation Split Per Seed
Each model gets a **different random validation split** because the train/val split uses `random_state=seed`:

```python
train_idx, val_idx = train_test_split(
    indices, 
    test_size=0.15, 
    stratify=stratify_labels, 
    random_state=seed  # Different for each model
)
```

This is **intentional for ensemble diversity**:
- Each model trains on slightly different data â†’ more diversity in learned features
- `stratify=stratify_labels` ensures each split maintains proportional location distribution
- The **test set** (`test_dataset.csv`) is external and fixed â†’ same for all models, fair comparison

### Expected Outcome
| Component | Expected Impact |
|-----------|-----------------|
| +131 problematic photos | ~10-15% error reduction |
| 320x320 resolution | ~5-10% improvement on fine details |
| Ensemble (3 models) | ~10-15% variance reduction |
| **Combined target** | 11.54m Ã— 0.75 â‰ˆ **8.7m** |

---

## Jan 2, 2026: Final Training Results - SUB-10M ACHIEVED! ðŸŽ‰

### Training Summary
Trained 3 models overnight on the GPU cluster (GTX 1080 Ti):
- **Duration**: ~5.5 hours total (22:21 - 03:44)
- **Epochs**: 200 per model
- **Batch size**: 32, num_workers=4, AMP enabled

| Model | Seed | Best Val Loss | Best Epoch |
|-------|------|---------------|------------|
| b0_320_seed42 | 42 | 5.6626 | ~134 |
| b0_320_seed123 | 123 | 6.1863 | ~134 |
| b0_320_seed456 | 456 | 5.6641 | 199 |

### Training Observations
- **Model 3 (seed=456)** was still improving at epoch 199 - saved a new best!
- Train/Val gap: ~3.2 vs ~5.7 (moderate overfitting, acceptable)
- Loss progression showed diminishing returns after epoch 100
- All models converged to similar final performance

### Final Test Set Results

#### Individual Models (1,023 test samples)
| Model | Mean Error | Median Error |
|-------|------------|--------------|
| b0_320_seed42 | 8.54m | 7.65m |
| b0_320_seed123 | 8.78m | 7.80m |
| b0_320_seed456 | 9.00m | 8.08m |

#### Ensemble Results
| Metric | Value |
|--------|-------|
| **Mean Error** | **8.17m** âœ… |
| **Median Error** | **7.33m** |
| Improvement over best individual | 0.37m (4.3%) |

### Night Holdout Results (54 samples)
| Metric | Value |
|--------|-------|
| Mean Error | 9.39m |
| Median Error | 7.32m |
| Min Error | 0.76m |
| Max Error | 26.30m |
| Under 10m | 59.3% |
| Under 20m | 90.7% |
| **Over 30m** | **0.0%** (no catastrophic failures!) |

### Worst Predictions Analysis

#### Test Set - Top 5 Worst
| Rank | Error | File |
|------|-------|------|
| 1 | 69.8m | TestPhotos_IMG_3104.jpg |
| 2 | 31.5m | TestPhotos_IMG_4378.jpg |
| 3 | 28.5m | TestPhotos_IMG_3412.jpg |
| 4 | 25.9m | TestPhotos_IMG_3696.jpg |
| 5 | 25.9m | TestPhotos_IMG_3506.jpg |

**Note**: Only 1 outlier >30m (69.8m) - likely a mislabeled or very ambiguous image.

#### Night Holdout - Top 5 Worst
| Rank | Error | File |
|------|-------|------|
| 1 | 26.3m | nightWithout26AndLibrary_IMG_6960.jpg |
| 2 | 24.4m | NightImagesLibraryArea_IMG_7489 2.jpg |
| 3 | 22.0m | nightWithout26AndLibrary_IMG_2545.jpg |
| 4 | 21.2m | nightWithout26AndLibrary_IMG_2550.jpg |
| 5 | 20.6m | nightWithout26AndLibrary_IMG_7800.jpg |

### Improvement Journey Summary

| Version | Mean Error | Median Error | Key Changes |
|---------|------------|--------------|-------------|
| v1 (EfficientNet-B0 baseline) | 15.27m | 15.10m | Initial model |
| v2 (b0_256_mlp) | 11.94m | 9.05m | MLP head, HuberLoss, augmentation |
| v3 (b0_256_v3 + ProblematicPhotos) | 11.54m | 8.46m | Active learning round 1 |
| **v4 (b0_320 ensemble)** | **8.17m** | **7.33m** | 320px, +131 photos, ensemble |

**Total improvement: 15.27m â†’ 8.17m (46% reduction!)**

### Key Techniques That Worked
1. **Active Learning (Problematic Photos)**: Moving high-error samples to training reduced blind spots
2. **Resolution Bump (320x320)**: More detail for distinguishing similar locations
3. **Ensembling**: 3 models with different seeds reduced variance by ~4%
4. **HuberLoss**: Robust to noisy GPS labels
5. **Data Augmentation**: Night simulation, perspective, rotation helped generalization

### Remaining Challenges
- 1 extreme outlier (69.8m) - investigate if mislabeled
- Night images still slightly harder (9.39m vs 8.17m overall)
- Some "generic architecture" views remain challenging

### Files Generated
- `outputs/worst_test_b0_320_seed42.png` - Test set worst 25 visualization
- `outputs/worst_night_b0_320_seed42.png` - Night holdout worst 25 visualization
- `checkpoints/best_b0_320_seed42.pth` - Best model (seed 42)
- `checkpoints/best_b0_320_seed123.pth` - Model 2 (seed 123)
- `checkpoints/best_b0_320_seed456.pth` - Model 3 (seed 456)

### Conclusion
**Goal achieved!** Sub-10m mean error with 8.17m ensemble performance. The Killer Combo Strategy worked as predicted, combining active learning, higher resolution, and ensembling for a robust final model.

---

**Date:** Jan 7, 2026

## Architecture Experiment: ConvNeXt-Tiny

### Motivation

After achieving 8.17m mean error with the EfficientNet-B0 ensemble, we analyzed potential paths forward:

1. **The Ensemble Limitation:** Our 3-model ensemble reduced *variance* but couldn't fix *bias*. All three models were EfficientNet-B0 with identical architectureâ€”they "think" the same way. If EfficientNet has a structural blind spot (e.g., struggles with textureless walls or long-distance relationships), all three models share it.

2. **Previous ConvNeXt Failure:** We had tried ConvNeXt-Tiny once before (Dec 27) but it failed catastrophically ("Loss exploded to ~4000+"). However, that was on Mac (MPS) with a small batch size and before we implemented many training improvements (HuberLoss, proper augmentation, AdamW).

3. **Why ConvNeXt Now?**
   - **Modern Architecture (2022):** ConvNeXt was designed to compete with Vision Transformers. It uses large 7Ã—7 kernels (vs EfficientNet's 3Ã—3), allowing it to "see" wider context and understand spatial relationships between landmarks.
   - **GPU Power Available:** With access to the GTX 1080 Ti cluster, we could train larger models properly.
   - **Proven Success:** ConvNeXt consistently outperforms EfficientNet on ImageNet benchmarks.

### Technical Changes

We created a new branch `experiment/convnext-tiny` to safely experiment.

#### 1. Architecture (`src/model/network.py`)

| Component | EfficientNet-B0 (Before) | ConvNeXt-Tiny (After) |
|-----------|-------------------------|----------------------|
| **Backbone** | `efficientnet_b0` (5.3M params) | `convnext_tiny` (28M params) |
| **Feature Channels** | 1280 | 768 |
| **Activation** | ReLU | GELU |
| **Head Structure** | Replace entire classifier | Replace only final Linear layer (preserve LayerNorm) |

```python
# Key change: Use GELU to match ConvNeXt's internal activation
regression_head = nn.Sequential(
    nn.Linear(768, 512),
    nn.GELU(),
    nn.Dropout(0.3),
    nn.Linear(512, 128),
    nn.GELU(),
    nn.Linear(128, 2)
)
self.backbone.classifier[2] = regression_head
```

#### 2. Training Configuration (`src/training/train.py`)

| Parameter | EfficientNet-B0 | ConvNeXt-Tiny | Reason |
|-----------|-----------------|---------------|--------|
| **Optimizer** | Adam | **AdamW** | AdamW decouples weight decay from gradient updatesâ€”standard for modern architectures |
| **Batch Size** | 32 | **24** | ConvNeXt uses more VRAM; 32 might OOM on 1080 Ti |
| **Learning Rate** | 1e-4 | 1e-4 | Kept same |
| **Weight Decay** | 1e-4 | 1e-4 | Kept same |
| **Epochs** | 200 | 100 | Initial run to check convergence |

#### 3. Data Pipeline
**No changes.** Used the same:
- 320Ã—320 resolution
- Same augmentation (ColorJitter, RandomPerspective, Night Simulation, RandomErasing)
- Same dataset (2,248 training samples)

### Training Results (100 Epochs)

**Training Command:**
```bash
nohup python3 -u -c "from src.training.train import train_model; train_model(num_epochs=100, experiment_name='convnext_tiny_v1', seed=42)" > training_convnext.log 2>&1 &
```

**Loss Progression:**

| Epoch | Train Loss | Val Loss | Notes |
|-------|------------|----------|-------|
| 1 | 45.03 | 38.89 | Starting point |
| 10 | 8.68 | 9.10 | Rapid improvement |
| 25 | 4.43 | 6.37 | Still learning |
| 50 | 3.13 | 5.05 | Continued progress |
| 75 | 2.87 | 4.65 | Getting close |
| 97 | 2.22 | **4.18** | â­ Best model saved |
| 100 | 2.18 | 4.30 | Slight overfitting |

**Key Observations:**
- Model was **still improving at epoch 100** (best saved at epoch 97)
- Train/Val gap: 2.2 vs 4.2 (~1.9x ratio) â€” healthy, indicates good generalization
- Final val loss (4.18) is **significantly lower** than EfficientNet-B0's best (5.66)

### Evaluation Results ðŸ†

#### Test Set (1,023 samples)

| Metric | EfficientNet-B0 (Best Single) | EfficientNet Ensemble (3 models) | **ConvNeXt-Tiny (Single)** |
|--------|-------------------------------|----------------------------------|---------------------------|
| **Mean Error** | 8.54m | 8.17m | **7.55m** |
| **Median Error** | 7.65m | 7.33m | **6.82m** |

**A single ConvNeXt model beats the entire EfficientNet ensemble!**

#### Night Holdout (54 samples)

| Metric | EfficientNet-B0 (Best) | **ConvNeXt-Tiny** | Improvement |
|--------|------------------------|-------------------|-------------|
| **Mean Error** | 9.39m | **7.43m** | **-20.9%** |
| **Median Error** | 7.32m | **6.66m** | **-9.0%** |
| **Under 10m** | 59.3% | **74.1%** | **+14.8%** |
| **Under 20m** | 90.7% | **96.3%** | **+5.6%** |
| **Over 30m** | 0.0% | **0.0%** | Maintained |
| **Max Error** | 26.30m | 29.75m | Slightly higher outlier |

#### Error Distribution (Test Set)

| Error Range | Count | Percentage |
|-------------|-------|------------|
| Under 5m | 327 | 32.0% |
| **Under 10m** | **772** | **75.5%** |
| Over 10m | 251 | 24.5% |
| Over 15m | 66 | 6.5% |
| Over 20m | 15 | 1.5% |
| Over 30m | 1 | 0.1% |

**Key insight:** 75.5% of predictions are within 10 meters. Only 1 sample (0.1%) has a catastrophic failure (>30m).

#### Worst Predictions Analysis

Visualized the 25 worst test predictions (`outputs/worst_test_convnext_tiny_v1.png`):

| Pattern | Count | Notes |
|---------|-------|-------|
| Covered walkways / pillars | ~10/25 | Generic concrete structures under buildings |
| Open plazas | ~8/25 | Wide views with few distinctive landmarks |
| Building 32 area | ~5/25 | The "30" sign building appears frequently |
| Long-distance views | ~4/25 | Far-away buildings that could be anywhere |

**Notable improvement:** Unlike EfficientNet (40-48% of worst were night images), ConvNeXt's worst 25 are almost all **daytime** images. Night performance is no longer a major weakness.

### Analysis: Why ConvNeXt Worked

1. **Larger Receptive Field:** ConvNeXt's 7Ã—7 kernels can see ~5x more context per layer than EfficientNet's 3Ã—3. This helps recognize "the library is on the left AND building 26 is on the right"â€”a spatial relationship that CNNs with small kernels struggle with.

2. **Night Vision Breakthrough:** The biggest improvement was on night images:
   - 74% of night predictions now under 10m (was 59%)
   - Mean night error dropped from 9.39m â†’ 7.43m
   - ConvNeXt's deeper features likely extract more robust low-light patterns

3. **Better Feature Hierarchy:** ConvNeXt uses LayerNorm (instead of BatchNorm) and GELU activation, which have been shown to produce smoother gradients and better generalization.

4. **No Overfitting (Yet):** Despite having 5x more parameters (28M vs 5.3M), the model didn't overfit badly. The train/val ratio of ~1.9x is acceptable. This suggests our augmentation pipeline is effective.

### Comparison: Journey So Far

| Version | Mean Error | Median Error | Key Changes |
|---------|------------|--------------|-------------|
| v1 (EfficientNet-B0 baseline) | 15.27m | 15.10m | Initial model |
| v2 (b0_256_mlp) | 11.94m | 9.05m | MLP head, HuberLoss, augmentation |
| v3 (b0_256_v3 + ProblematicPhotos) | 11.54m | 8.46m | Active learning round 1 |
| v4 (b0_320 ensemble) | 8.17m | 7.33m | 320px, +131 photos, ensemble |
| **v5 (ConvNeXt-Tiny single)** | **7.55m** | **6.82m** | Architecture upgrade |

**Total improvement: 15.27m â†’ 7.55m (50.5% reduction!)**

### Files Generated
- `checkpoints/best_convnext_tiny_v1.pth` - Best ConvNeXt model (epoch 97)
- `training_convnext.log` - Full training log (on cluster)

### Next Steps (To Discuss)

1. **Ensemble ConvNeXt:** Train 2 more ConvNeXt models (seeds 123, 456) and ensemble. Expected: ~6.5-7.0m mean error.
2. **Continue Training:** The model was still improving at epoch 100. Resume for 50+ more epochs?
3. **Hybrid Ensemble:** Combine 2 ConvNeXt + 1 EfficientNet for maximum diversity?
4. **Merge Branch:** Bring these improvements to main.

---

## Test Set GPS Corrections (Batch 1)

**Date:** Jan 7, 2026

### Motivation

After visualizing ConvNeXt's 40 worst predictions, we noticed some images had suspiciously bad GPS labels (phone GPS drift). Instead of assuming all errors were model mistakes, we manually verified each image's location using Google Maps.

### Process

1. **Exported worst 40 predictions** to `outputs/worst_for_maps.csv` with image names and predicted GPS coordinates
2. **Visualized all 40 images** in order (`outputs/worst_40_convnext.png`) for reference
3. **Manually verified each location** in Google Maps, dragging pins to correct positions
4. **Saved corrections** to `data/test_corrections_convnext_v1.csv`

### Correction Results

Of the 40 images examined:
- **16 images (40%)** had correct GPS â€” ConvNeXt was actually wrong on these
- **24 images (60%)** had bad GPS that we corrected â€” ConvNeXt may have been right!

Notable corrections:
| Image | GPS Movement | Notes |
|-------|--------------|-------|
| TestPhotos_IMG_3768.jpg | 40.8m | Largest correction |
| TestPhotos_IMG_4267.jpg | 27.1m | Significant drift |
| TestPhotos_IMG_4378.jpg | 19.5m | Phone GPS error |
| TestPhotos_IMG_3298.jpg | 19.8m | Misplaced point |

### Impact on Metrics

Applied corrections directly to `test_dataset.csv` (test set is a fixed holdout, we're just fixing bad labels):

| Metric | Before Corrections | After Corrections | Change |
|--------|-------------------|-------------------|--------|
| **Mean Error** | 7.55m | **7.46m** | -0.09m |
| **Median Error** | 6.82m | 6.82m | (same) |

**Error Distribution Changes:**

| Range | Before | After | Change |
|-------|--------|-------|--------|
| Under 10m | 772 (75.5%) | 775 (75.8%) | +3 |
| Over 15m | 66 (6.5%) | 57 (5.6%) | **-9** |
| Over 20m | 15 (1.5%) | 12 (1.2%) | **-3** |
| Over 30m | 1 (0.1%) | **0 (0.0%)** | **-1** âœ“ |

**Key win:** No more catastrophic failures (0 samples over 30m error).

### Files Generated
- `data/test_corrections_convnext_v1.csv` â€” 40 manually verified GPS corrections
- Updated `data/test_dataset.csv` â€” Test set with corrected labels

### Insight

This exercise revealed that ~60% of our "worst" predictions were actually caused by **bad GPS labels**, not model errors. The model was often predicting correctly, but being penalized for phone GPS drift. This validates our data-centric approach: cleaning labels is just as important as improving the model.

---

## Experiment: Swin Transformer & Final ConvNeXt Ensemble

**Date:** Jan 8, 2026

### 1. Swin Transformer Experiment
To add architectural diversity to our ensemble, we trained a `Swin-T` (Vision Transformer).
*   **Hypothesis:** Transformers process images globally (patches + attention) rather than locally (sliding windows), potentially catching cues that CNNs miss.
*   **Result:** **Failure (Overfitting)**
    *   Test Mean Error: **9.68m** (vs 7.46m for ConvNeXt)
    *   Train Loss: 3.49 vs Val Loss: 5.90 (Ratio 0.59x â†’ Heavy Overfitting)
*   **Conclusion:** Swin Transformers likely require more data (millions of images) to generalize well. For our ~2k image dataset, the inductive bias of CNNs (ConvNeXt) is superior. We abandoned Swin for the final model.

### 2. ConvNeXt Ensemble Strategy
Since ConvNeXt-Tiny was the clear winner, we decided to maximize its performance by training 3 variants with different random seeds. This reduces variance and typically improves accuracy by 5-10%.

**Models Trained:**
1.  `convnext_tiny_v1` (Seed 42) - The original success
2.  `convnext_tiny_v2` (Seed 123) - 125 epochs
3.  `convnext_tiny_v3` (Seed 456) - 125 epochs
4.  `convnext_tiny_v4` (Seed 789) - Stopped early (Epoch 19), discarded.

**Individual Performance (Test Set):**
| Model | Seed | Mean Error | Median Error |
|-------|------|------------|--------------|
| v1 | 42 | 7.46m | 6.82m |
| v2 | 123 | 7.69m | 6.68m |
| v3 | 456 | 7.54m | 6.72m |

*Note: All single models performed consistently (~7.5m mean error).*

### 3. Final Ensemble Results
We combined the predictions of v1, v2, and v3 (averaging lat/lon outputs).

| Configuration | Mean Error | Median Error | Improvement |
|---------------|------------|--------------|-------------|
| Best Single (v1) | 7.46m | 6.82m | - |
| **Final Ensemble** | **7.16m** | **6.48m** | **-0.30m (4.0%)** |

**Conclusion:**
We have reached a mean error of **7.16m**. The ensemble successfully smoothed out individual model variance. Given the GPS accuracy of standard smartphones is ~3-5m, we are likely approaching the theoretical limit of what is possible with this dataset and ground truth labels.

### 4. "The Impossible 14" Analysis
We intersected the "Top 50 Worst Predictions" from all 3 ConvNeXt variants to find images that are **universally hard** (failed by all seeds).

**Results:** Found **14 images** (~1.3% of test set) that consistently fail across all models.

| Filename | Avg Error | Notes |
|----------|-----------|-------|
| `TestPhotos_IMG_3862.jpg` | **28.8m** | Worst across all models. |
| `TestPhotos_IMG_3532.jpg` | 27.5m | GPS correction made this *worse* (+10m). Visual aliasing likely. |
| `TestPhotos_IMG_3579.jpg` | 23.9m | High variance but always bad. |
| `TestPhotos_IMG_3882.jpg` | 22.1m | Another case where correction didn't help. |

**Insight:**
The `IMG_38xx` series appears 5 times in the top 7 hard images. This suggests a systematic issue with a specific location (e.g., highly repetitive architecture or remaining bad labels).

**Future Work:**
These 14 images are prime candidates for:
1.  **Manual Re-verification:** Double-check their GPS coordinates on-site if possible.
2.  **Training Inclusion:** Move them from Test to Train set (if they represent a rare view).
3.  **Blacklisting:** If they are truly ambiguous (e.g., a blank wall), remove them to avoid misleading metrics.

### 5. Hybrid Ensemble Experiment (Bonus)
We tested if mixing architectures (ConvNeXt + EfficientNet) yields better diversity than mixing seeds.

| Configuration | Composition | Mean Error | Median Error |
|---------------|-------------|------------|--------------|
| **Hybrid Ensemble** | 2 ConvNeXt + 1 EfficientNet | **7.13m** | **6.33m** |
| Pure ConvNeXt | 3 ConvNeXt | 7.16m | 6.48m |
| Pure EfficientNet | 3 EfficientNet | 8.12m | 7.33m |

**Result:** Replacing one ConvNeXt with an EfficientNet (`seed42`) improved performance slightly (-0.03m mean, -0.15m median).
**Decision:** We have the option to use the Hybrid ensemble for the absolute best numbers, or the Pure ConvNeXt ensemble for code simplicity. The gain is marginal (3cm).

### Next Steps
1.  **Merge** the experiment branch to main.
2.  **Final Clean-up:** Remove temporary experiment files.
3.  **Documentation:** Update the main README with final metrics.
